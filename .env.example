# ============================================
# PHASE 1: DATA COLLECTION (Start Here!)
# ============================================

# Discord Bot Credentials
DISCORD_BOT_TOKEN=your_bot_token_here
DISCORD_GUILD_ID=your_server_id_here

# IMPORTANT: Single channel operation
# The bot will ONLY collect data from this ONE channel
# To get channel ID: Right-click channel → Copy Channel ID (requires Developer Mode in Discord settings)
DISCORD_CHANNEL_ID=your_channel_id_here

# Data Collection Settings
REQUESTS_PER_SECOND=40  # Fast collection (within Discord limits, can reduce to 20 if rate limited)
MAX_MESSAGES=65000  # Collect 65k (expect ~63k after filtering)
ENABLE_CHECKPOINT=true  # Resume if interrupted
CHECKPOINT_INTERVAL=1000  # Save progress every 1000 messages

# ============================================
# PHASE 2: TRAINING (Configure later)
# ============================================

# Training Data
TRAINING_DATA_PATH=./data/training_data.jsonl

# Training Settings (OPTIMIZED for RTX 2080 Ti - 13-18 hour training!)
LEARNING_RATE=2.5e-4  # Adjusted for larger effective batch size
EPOCHS=1  # For 63k pre-filtered messages - DO NOT increase (overfitting risk)
BATCH_SIZE=1
GRADIENT_ACCUMULATION_STEPS=32  # Increased from 16 for 2× speedup
GRADIENT_CHECKPOINTING=true  # Required for 11GB VRAM during training
USE_FP16=true  # Mixed precision training for 1.3× speedup (RTX 2080 Ti compatible)

# Result: 13-18 hours instead of 35 hours! ⚡
# Quality: Identical to baseline (zero quality loss)

# ============================================
# PHASE 3: DEPLOYMENT (Configure later)
# ============================================

# Model Configuration
MODEL_NAME=mistralai/Mistral-7B-v0.3
ADAPTER_PATH=./adapters/discord-lora
LOAD_IN_4BIT=true  # CRITICAL for 11GB VRAM
DEVICE=cuda  # Options: cuda, cpu, mps (for Mac)

# Response Generation Settings
CONTEXT_TOKENS=384  # Reduced for 11GB VRAM (was 512)
MAX_NEW_TOKENS=150  # Response length in tokens (100=short, 150=balanced, 200=long)
TEMPERATURE=0.7  # Randomness (0.5=focused, 0.7=balanced, 0.9=creative)
TOP_P=0.9  # Nucleus sampling (0.9=standard, higher=more diverse)
TOP_K=40  # Top-k sampling (40=standard, higher=more diverse)
REPETITION_PENALTY=1.1  # Avoid repetition (1.0=none, 1.1=light, 1.3=strong)

# Short-Term Memory Settings
SHORT_TERM_WINDOW=20  # Number of recent messages to remember per channel (10-50 recommended)
SHORT_TERM_MAX_CONTEXT=10  # Max messages to include in prompt (5-15 recommended, lower=faster)

# Long-Term Memory Settings (RAG)
ENABLE_LONG_TERM_MEMORY=false  # Set to true to enable persistent memory
LONG_TERM_DB_PATH=./data/vector_db  # ChromaDB database location
LONG_TERM_TOP_K=3  # Number of relevant memories to retrieve (1-5 recommended)
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2  # Model for embeddings

# Summarization Settings
ENABLE_SUMMARIZATION=false  # Set to true to enable conversation summaries
SUMMARIZATION_MODEL=facebook/bart-large-cnn  # Model for summarization
SUMMARIZE_THRESHOLD=30  # Summarize when context exceeds this many messages
SUMMARY_MAX_LENGTH=150  # Maximum length of summaries in tokens
SUMMARY_MIN_LENGTH=40  # Minimum length of summaries in tokens

# System Settings
LOG_LEVEL=INFO  # Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_FILE=./logs/bot.log
CHECKPOINT_PATH=./data/backfill_checkpoint.json

# Performance
ENABLE_QUANTIZATION=true  # MUST BE TRUE for 11GB VRAM
USE_FLASH_ATTENTION=false  # Set to true if you install flash-attn (faster inference, requires compilation)
